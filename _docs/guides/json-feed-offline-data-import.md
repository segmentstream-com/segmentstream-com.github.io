---
layout: page
section: guides
navigation_title: "Importing transactions or leads using JSON feed"
title: "Importing transactions or leads using JSON feed"
date: 2020-05-08
---

<ul class="page-navigation">
  <li><a href="#before-you-begin">Before you begin</a></li>
  <li><a href="#creating-feed">Step 1: Creating JSON feed</a>
    <ul>
      <li><a href="#feed-requirements">Feed requirements</a></li>
      <li><a href="#feed-example">Transaction feed example</a></li>
      <li><a href="#how-feed-import-works">How feed import works</a></li>
      <li><a href="#all-orders">Approach 1: all orders</a></li>
      <li><a href="#daily-updates">Approach 2: daily updates</a></li>
      <li><a href="#feed-contents">Feed contents</a></li>
    </ul>
  </li>
  <li><a href="#enabling-feed-import">Step 2: Enabling feed import in SegmentStream</a></li>
</ul>
This document describes how to import offline data (transactions or leads) into BigQuery using JSON feed.

SegmentStream is a platform that allows you to automatically collect marketing data from all your sources to BigQuery to enable advanced ROI reporting & attribution. 

Importing offline transactions can be used with other website data to improve ROI reporting.

## <a name="before-you-begin"></a> Before you begin
* Make sure that BigQuery is [Connected to SegmentStream](/_docs/bigquery/connecting-bigquery).

## <a name="creating-feed"></a> Step 1: Creating JSON feed 

SegmentStream can download feeds in JSON format and import their content into Google BigQuery.

### <a name="#feed-requirements"></a>Feed requirements
* The feed should be generated by the customer’s back-end and be available at a specific URL, for example, `https://example.com/feed.json`.

* The feed should be in [Newline Delimited JSON file format](http://ndjson.org/).

* We highly recommend implementing [basic Auth](https://en.wikipedia.org/wiki/Basic_access_authentication) to protect feed from being publicly exposed.
* (Optional, recommended) It should be possible to fetch daily feed data using date parameter added to the feed URL.

**Example:**
`https://example.com/feed.json?date=20200101` - displays transaction updates for 1st of January 2020
This way instead of importing all transactions, only daily updates will be imported. This approach will be explained later on in this guide.

### <a name="#feed-example"></a>Transaction feed example (formatted JSON version)
```json
[{
    "createdAt": "2020-01-20T20:00:20.202Z",
    "updatedAt": "2020-01-20T20:00:20.202Z",
    "orderId": "100",
    "currency": "USD",
    "total": 120.13,
    "status": "created",
    "userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"
},
{
    "createdAt": "2020-01-19T06:56:12.554Z",
    "updatedAt": "2020-01-20T20:02:01.301Z",
    "orderId": "50",
    "currency": "USD",
    "total": 96.12,
    "status": "shipped",
    "userId":"86116bb2-716b-4762-bc38-82417e69f7a1"
},
{
    "createdAt": "2020-01-19T10:53:40.591Z",
    "updatedAt": "2020-01-20T20:02:02.711Z",
    "orderId": "73",
    "currency": "USD",
    "total": 299.99,
    "status": "pendingShipment",
    "userId":"dee3ca08-27de-495c-8738-060029a58449"
}]
```
> Note: This feed is JSON formatted - not NDJSON - don't format your feed like this.

The previous feed looks like this when formatted in NDJSON format:

```ndjson
{"createdAt":"2020-01-20T20:00:20.202Z","updatedAt":"2020-01-20T20:00:20.202Z","orderId":"100","currency":"USD","total":120.13,"status":"created","userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"}

{"createdAt":"2020-01-19T06:56:12.554Z","updatedAt":"2020-01-20T20:02:01.301Z","orderId":"50","currency":"USD","total":96.12,"status":"shipped","userId":"86116bb2-716b-4762-bc38-82417e69f7a1"}

{"createdAt":"2020-01-19T10:53:40.591Z","updatedAt":"2020-01-20T20:02:02.711Z","orderId":"73","currency":"USD","total":299.99,"status":"pendingShipment","userId":"dee3ca08-27de-495c-8738-060029a58449"}
```

Where: 
* `createdAt` - time of the order creation in ISO 8601 standard
* `updatedAt` - time of the latest order status update in ISO 8601
* `orderId` - unique identifier of the order in your CRM system or website database. For other businesses it might be lead identifier or any other unique record identifier.
* `currency` - order currency code
* `total` - Total cost of the order
* `status` - Order status by the time of  (e.g. "created", "shipped", "cancelled", "pendingPayment", "pendingShipment", "returned")
* `userId` - unique identifier of the user that made an order
> Note: the structure is provided as a reference - you can add any other field that you need.

### <a name="#how-feed-import-works"></a> How feed import works
Every day SegmentStream fetches data from the specified feed URL using credentials provided and appending date parameter to the  URL.
The date parameter is formatted as `YYYYMMDD` where:

`YYYY` - current year,
`MM` - current month, 
`DD` - current date.

Example of the possible feed request as of 9 May 2020: 
<code>
https://example.com/feed.json?**date=20200509**
</code>

You can implement one of the two approaches regarding returning feed data on request:

### <a name="#all-orders"></a> Approach 1: Returning all of the order records on each request
If you implement this approach every day your feed data in BigQuery will be overwritten by all of the transactions from your feed.

Sample feed:


line|feed record|
--- | --- |
1 | `{"createdAt":"2020-01-19T20:00:20.202Z","updatedAt":"2020-01-20T20:00:20.202Z","orderId":"100","currency":"USD","total":120.13,"status":"created","userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"}`
2 | `{"createdAt":"2020-01-19T20:00:20.202Z","updatedAt":"2020-01-21T05:03:15.661Z","orderId":"100","currency":"USD","total":120.13,"status":"shipped","userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"}`
3| `{"createdAt":"2020-01-19T10:53:40.591Z","updatedAt":"2020-01-22T18:15:34.031Z","orderId":"73","currency":"USD","total":299.99,"status":"pendingShipment","userId":"dee3ca08-27de-495c-8738-060029a58449"}`

> **Notice** that feed contains different update date value (stored in `updatedAt` field) - from 20th to 22th Jan 2020.

If you implement this approach for every  value of the `date` URL parameter you should return all of the order records.

### <a name="#daily-updates"></a> Approach 2: Returning daily transaction updates partitioned by day

If you choose to implement this option you should parse `date` GET parameter of the feed request and respond only with necessary update records.
e.g. if request is `https://example.com?date=20200120`
you should return only records that were made on the 20th Jan 2020:

line|feed record|
--- | --- |
1|`{"createdAt":"2020-01-20T20:00:20.202Z","updatedAt":"2020-01-20T20:00:20.202Z","orderId":"100","currency":"USD","total":120.13,"status":"created","userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"}`
2|`{"createdAt":"2020-01-19T06:56:12.554Z","updatedAt":"2020-01-20T20:02:01.301Z","orderId":"50","currency":"USD","total":96.12,"status":"shipped","userId":"86116bb2-716b-4762-bc38-82417e69f7a1"}`
3|`{"createdAt":"2020-01-19T10:53:40.591Z","updatedAt":"2020-01-20T20:02:02.711Z","orderId":"73","currency":"USD","total":299.99,"status":"pendingShipment","userId":"dee3ca08-27de-495c-8738-060029a58449"}`


> Note: This approach allows you to reduce BigQuery costs when implementing ROI reporting as only daily updates would be processed instead of processing the whole feed table.

### <a name="#feed-contents"></a> Feed contents
Your order feed should contain update records, meaning that for each change in the status of existing transaction you create a new record in the feed with the new value of the `updatedAt` field and preserving the value of the `createdAt` field. 

**Example:**
Order was created on the 19th Jan 2020 and later it was shipped on the 20th Jan 2020.
It should be reflected in the feed the following way:
>Note: this is not valid NDJSON as it contains comments.

line|feed record|
--- | --- |
1| `{"createdAt":"2020-01-20T20:00:20.202Z","updatedAt":"2020-01-20T20:00:20.202Z","orderId":"100","currency":"USD","total":120.13,"status":"created","userId":"4af3ce14-e8ab-4c7a-907d-3848e6bdb7cf"}`
2| `{"createdAt":"2020-01-20T20:00:20.202Z","updatedAt":"2020-01-21T17:02:01.301Z","orderId":"50","currency":"USD","total":96.12,"status":"shipped","userId":"86116bb2-716b-4762-bc38-82417e69f7a1"}`
 
## <a name="enabling-feed-import"></a> Step 2: Enabling feed import in SegmentStream
1. Go to [SegmentStream admin page ▸](https://admin.segmentstream.com)
2. Open **Data Sources ▸ Add**
3. Select **JSON Feed**
4. Fill in Basic Auth credentials
5. Fill in the fields:
    * **Feed URL** - fill in URL where your feed can be found (don't enter date parameter)
    * **Destination table name** - BigQuery table name that will contain imported feed data
6. If you use date  parameter and generate daily record updates enable option **Partition table by date**

Congratulations! During the next 24 hours your feed data will be uploaded to the corresponding BigQuery table.